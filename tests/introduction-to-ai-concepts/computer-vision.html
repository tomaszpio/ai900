<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI-900 Practice Test — Computer Vision</title>
<style>
  :root {
    color-scheme: light;
    --page-max-width: 64rem;
    --surface: #ffffff;
    --surface-alt: #f8f9fb;
    --border: #d0d7de;
    --accent: #004578;
    --accent-soft: #f1f8ff;
  }

  *,
  *::before,
  *::after {
    box-sizing: border-box;
  }

  body {
    font-family: "Segoe UI", sans-serif;
    background: var(--surface-alt);
    color: #222;
    margin: 0;
    line-height: 1.6;
    padding: clamp(1rem, 4vw, 2.5rem);
  }

  .return-home {
    position: fixed;
    top: clamp(0.75rem, 2vw, 1.25rem);
    right: clamp(0.75rem, 3vw, 1.5rem);
    z-index: 1000;
    display: inline-flex;
    align-items: center;
    gap: 0.45rem;
    padding: 0.55rem 1.05rem;
    background: var(--accent, #004578);
    color: #ffffff;
    border-radius: 999px;
    text-decoration: none;
    font-weight: 600;
    font-size: 0.95rem;
    border: 1px solid rgba(255, 255, 255, 0.55);
    box-shadow: 0 10px 24px rgba(0, 69, 120, 0.22);
    transition: transform 0.2s ease, box-shadow 0.2s ease, background-color 0.2s ease;
  }

  .return-home svg {
    width: 1em;
    height: 1em;
    fill: currentColor;
  }

  .return-home:hover,
  .return-home:focus-visible {
    background: var(--accent-strong, #00345c);
    box-shadow: 0 14px 30px rgba(0, 69, 120, 0.28);
    transform: translateY(-1px);
  }

  @media (max-width: 600px) {
    .return-home {
      font-size: 0.85rem;
      padding: 0.5rem 0.9rem;
      top: 0.75rem;
      right: 0.75rem;
    }
  }

  body > * {
    width: min(100%, var(--page-max-width));
    margin: 0 auto;
  }

  h1 {
    color: var(--accent);
    margin-bottom: clamp(1.25rem, 4vw, 1.75rem);
    font-size: clamp(1.75rem, 4vw, 2.25rem);
  }

  .question {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 10px;
    padding: clamp(0.9rem, 2.5vw, 1.35rem);
    margin-bottom: clamp(1rem, 3vw, 1.5rem);
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.05);
  }

  .question h2 {
    font-size: clamp(1.05rem, 3vw, 1.2rem);
    margin-bottom: 0.5rem;
  }

  ul.options {
    list-style-type: upper-alpha;
    margin: 0;
    padding-left: clamp(1.1rem, 5vw, 1.5rem);
  }

  ul.options li {
    margin-bottom: 0.4rem;
  }

  details {
    margin-top: 0.65rem;
  }

  summary {
    cursor: pointer;
    font-weight: 600;
    color: #0063b1;
  }

  summary:focus-visible {
    outline: 3px solid rgba(0, 99, 177, 0.35);
    border-radius: 6px;
  }

  .answer {
    background: var(--accent-soft);
    border-left: 4px solid #0063b1;
    padding: clamp(0.75rem, 2.5vw, 1rem) clamp(0.85rem, 3vw, 1.15rem);
    margin-top: 0.5rem;
    border-radius: 6px;
  }

  @media (max-width: 600px) {
    body {
      padding: clamp(1rem, 6vw, 1.5rem);
    }
  }

  @media (max-width: 420px) {
    ul.options {
      padding-left: 1rem;
    }
  }
</style>
</head>
<body>
<a class="return-home" href="../../index.html">
  <svg aria-hidden="true" viewBox="0 0 24 24" focusable="false">
    <path d="M12 4.248 4.5 10v10h5v-5h5v5h5V10z"></path>
  </svg>
  <span>Home</span>
</a>

<h1>AI-900 Practice Test — Computer Vision</h1>

<!-- Q1 -->
<div class="question">
  <h2>1️⃣ What is the primary goal of computer vision?</h2>
  <ul class="options">
    <li>A. To interpret and understand visual information from images or video.</li>
    <li>B. To translate speech to text.</li>
    <li>C. To predict future sales.</li>
    <li>D. To generate marketing emails.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> A — To interpret and understand visual information from images or video.<br>
    <strong>Explanation:</strong> Computer vision solutions extract meaning from visual inputs so applications can reason over them.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/computer-vision/overview">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q2 -->
<div class="question">
  <h2>2️⃣ Which Azure service provides prebuilt and customizable vision capabilities?</h2>
  <ul class="options">
    <li>A. Azure AI Vision</li>
    <li>B. Azure Synapse Analytics</li>
    <li>C. Azure Data Explorer</li>
    <li>D. Azure DevOps</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> A — Azure AI Vision.<br>
    <strong>Explanation:</strong> Azure AI Vision delivers prebuilt models and custom vision options for analyzing images and videos.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/computer-vision/overview">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q3 -->
<div class="question">
  <h2>3️⃣ Which workload best matches <em>image classification</em>?</h2>
  <ul class="options">
    <li>A. Predicting numerical values over time.</li>
    <li>B. Identifying which label best describes an entire image.</li>
    <li>C. Translating text between languages.</li>
    <li>D. Summarizing meeting transcripts.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — Identifying which label best describes an entire image.<br>
    <strong>Explanation:</strong> Image classification assigns a single category to an image from a predefined set of labels.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/custom-vision-service/classify-images">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q4 -->
<div class="question">
  <h2>4️⃣ What is <em>object detection</em> used for?</h2>
  <ul class="options">
    <li>A. Detecting anomalies in time-series data.</li>
    <li>B. Locating and labeling multiple objects within an image.</li>
    <li>C. Recognizing spoken commands.</li>
    <li>D. Generating captions from prompts.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — Locating and labeling multiple objects within an image.<br>
    <strong>Explanation:</strong> Object detection models return bounding boxes and class labels for each detected object.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/custom-vision-service/object-detection-overview">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q5 -->
<div class="question">
  <h2>5️⃣ Which scenario best fits <em>semantic segmentation</em>?</h2>
  <ul class="options">
    <li>A. Classifying entire documents as spam or not.</li>
    <li>B. Assigning a class label to every pixel in an image.</li>
    <li>C. Translating captions into multiple languages.</li>
    <li>D. Predicting website traffic.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — Assigning a class label to every pixel in an image.<br>
    <strong>Explanation:</strong> Semantic segmentation provides dense labeling to show how regions of an image relate to each other.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/custom-vision-service/semantic-segmentation">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q6 -->
<div class="question">
  <h2>6️⃣ How does Azure AI Vision's <em>Image Analysis</em> feature help developers?</h2>
  <ul class="options">
    <li>A. It schedules distributed training jobs.</li>
    <li>B. It extracts tags, descriptions, and detected objects from images.</li>
    <li>C. It encrypts data in transit.</li>
    <li>D. It manages container registries.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — It extracts tags, descriptions, and detected objects from images.<br>
    <strong>Explanation:</strong> Image Analysis applies prebuilt models to understand image content without custom training.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/computer-vision/overview-image-analysis">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q7 -->
<div class="question">
  <h2>7️⃣ When should you use <em>custom vision</em> capabilities?</h2>
  <ul class="options">
    <li>A. When prebuilt models already recognize your domain-specific objects.</li>
    <li>B. When you need to train models on your own labeled images for specialized scenarios.</li>
    <li>C. When you do not have any labeled image data.</li>
    <li>D. When you want to translate text documents.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — When you need to train models on your own labeled images for specialized scenarios.<br>
    <strong>Explanation:</strong> Custom vision lets you upload and label data to build classification or detection models tailored to your use case.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/custom-vision-service/overview">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q8 -->
<div class="question">
  <h2>8️⃣ What is required to train a custom vision model in Azure?</h2>
  <ul class="options">
    <li>A. A large language model prompt.</li>
    <li>B. Labeled training images grouped into tags.</li>
    <li>C. SQL stored procedures.</li>
    <li>D. GPU-enabled virtual machines in the customer subscription.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — Labeled training images grouped into tags.<br>
    <strong>Explanation:</strong> Custom vision services learn from labeled examples to differentiate between classes.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/custom-vision-service/overview">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q9 -->
<div class="question">
  <h2>9️⃣ What does Optical Character Recognition (OCR) return when analyzing an image of text?</h2>
  <ul class="options">
    <li>A. The dominant colors in the image.</li>
    <li>B. Detected text content, layout information, and bounding boxes.</li>
    <li>C. A summary of the document’s key phrases.</li>
    <li>D. A speech transcript of spoken words.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — Detected text content, layout information, and bounding boxes.<br>
    <strong>Explanation:</strong> OCR extracts textual data and spatial location so applications can search and reuse printed or handwritten text.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/computer-vision/overview-ocr">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q10 -->
<div class="question">
  <h2>🔟 Which scenario is a good fit for Azure AI Vision <em>Spatial Analysis</em>?</h2>
  <ul class="options">
    <li>A. Tracking customer sentiment on social media.</li>
    <li>B. Counting people entering and exiting a retail store in real time.</li>
    <li>C. Translating scanned documents.</li>
    <li>D. Classifying support tickets by topic.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — Counting people entering and exiting a retail store in real time.<br>
    <strong>Explanation:</strong> Spatial Analysis uses video streams to understand movements and occupancy patterns.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/computer-vision/spatial-analysis-feature-overview">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q11 -->
<div class="question">
  <h2>1️⃣1️⃣ How does object detection differ from image classification?</h2>
  <ul class="options">
    <li>A. Detection assigns a single label to the entire image.</li>
    <li>B. Detection identifies and localizes multiple objects, whereas classification labels an image as a whole.</li>
    <li>C. Detection is only for grayscale images.</li>
    <li>D. Detection requires no training data.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — Detection identifies and localizes multiple objects, whereas classification labels an image as a whole.<br>
    <strong>Explanation:</strong> Object detection returns bounding boxes plus labels, providing richer detail than simple classification.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/custom-vision-service/object-detection-overview">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q12 -->
<div class="question">
  <h2>1️⃣2️⃣ What is a <em>bounding box</em> in computer vision?</h2>
  <ul class="options">
    <li>A. A container that stores training datasets.</li>
    <li>B. A rectangular region that outlines a detected object within an image.</li>
    <li>C. An Azure Storage account configured for images.</li>
    <li>D. A compression method for reducing image size.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — A rectangular region that outlines a detected object within an image.<br>
    <strong>Explanation:</strong> Detection algorithms output bounding boxes to describe an object's location and size.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/custom-vision-service/object-detection-overview">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q13 -->
<div class="question">
  <h2>1️⃣3️⃣ Which Azure service specializes in extracting structured data from documents using vision and language?</h2>
  <ul class="options">
    <li>A. Azure AI Document Intelligence (Form Recognizer)</li>
    <li>B. Azure Databricks</li>
    <li>C. Azure Monitor</li>
    <li>D. Azure VPN Gateway</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> A — Azure AI Document Intelligence (Form Recognizer).<br>
    <strong>Explanation:</strong> Document Intelligence combines OCR and layout understanding to extract key-value pairs and tables.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/document-intelligence/overview">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q14 -->
<div class="question">
  <h2>1️⃣4️⃣ Why is <em>data privacy</em> important in computer vision scenarios?</h2>
  <ul class="options">
    <li>A. Vision systems never capture sensitive information.</li>
    <li>B. Images and videos may include personal or identifiable data that must be protected.</li>
    <li>C. Compliance rules do not apply to visual data.</li>
    <li>D. Privacy considerations slow model training.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — Images and videos may include personal or identifiable data that must be protected.<br>
    <strong>Explanation:</strong> Responsible AI practices ensure visual data is collected, stored, and used ethically and legally.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/responsible-use-of-ai/computer-vision">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q15 -->
<div class="question">
  <h2>1️⃣5️⃣ How can you deploy Azure AI Vision capabilities to run close to your data?</h2>
  <ul class="options">
    <li>A. By using Azure AI Vision Docker containers.</li>
    <li>B. By enabling Azure Active Directory.</li>
    <li>C. By exporting the model to Power BI dashboards.</li>
    <li>D. By converting the model to SQL views.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> A — By using Azure AI Vision Docker containers.<br>
    <strong>Explanation:</strong> Containers let you run Vision capabilities on-premises or at the edge while managing data locality.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/computer-vision/computer-vision-how-to-install-containers">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q16 -->
<div class="question">
  <h2>1️⃣6️⃣ What insight does Azure Video Indexer provide for stored video content?</h2>
  <ul class="options">
    <li>A. CPU usage metrics for virtual machines.</li>
    <li>B. Detects faces, transcripts speech, and identifies scenes for searchable metadata.</li>
    <li>C. Converts videos to spreadsheets.</li>
    <li>D. Schedules build pipelines.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — Detects faces, transcripts speech, and identifies scenes for searchable metadata.<br>
    <strong>Explanation:</strong> Video Indexer enriches video libraries with AI-generated insights for discovery and analysis.
    Learn more in the <a href="https://learn.microsoft.com/azure/azure-video-indexer/video-indexer-overview">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q17 -->
<div class="question">
  <h2>1️⃣7️⃣ Which evaluation metric is especially important for object detection models?</h2>
  <ul class="options">
    <li>A. Mean Average Precision (mAP).</li>
    <li>B. Disk throughput.</li>
    <li>C. Network bandwidth.</li>
    <li>D. Power consumption.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> A — Mean Average Precision (mAP).<br>
    <strong>Explanation:</strong> mAP measures how accurately the model identifies and localizes objects across confidence thresholds.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/custom-vision-service/evaluate-object-detection-model">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q18 -->
<div class="question">
  <h2>1️⃣8️⃣ How can active learning improve a computer vision model over time?</h2>
  <ul class="options">
    <li>A. By automatically building web dashboards.</li>
    <li>B. By prioritizing uncertain predictions for human review and new labels.</li>
    <li>C. By increasing GPU clock speeds.</li>
    <li>D. By caching API responses.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — By prioritizing uncertain predictions for human review and new labels.<br>
    <strong>Explanation:</strong> Active learning focuses labeling effort on ambiguous examples, improving accuracy with less data.
    Learn more in the <a href="https://learn.microsoft.com/azure/machine-learning/concept-active-learning">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q19 -->
<div class="question">
  <h2>1️⃣9️⃣ Why is human oversight important in computer vision solutions?</h2>
  <ul class="options">
    <li>A. Vision models are infallible.</li>
    <li>B. Humans validate predictions, catch errors, and uphold responsible AI principles.</li>
    <li>C. Oversight increases storage costs.</li>
    <li>D. It eliminates the need for testing.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — Humans validate predictions, catch errors, and uphold responsible AI principles.<br>
    <strong>Explanation:</strong> Human review ensures model outputs remain accurate, fair, and aligned with business rules.
    Learn more in the <a href="https://learn.microsoft.com/azure/ai-services/responsible-use-of-ai/lifecycle#human">Microsoft documentation</a>.
  </div></details>
</div>

<!-- Q20 -->
<div class="question">
  <h2>2️⃣0️⃣ How should you secure keys for Azure AI Vision and custom vision resources?</h2>
  <ul class="options">
    <li>A. Store them directly in client-side JavaScript.</li>
    <li>B. Rotate and store them in secure services such as Azure Key Vault.</li>
    <li>C. Share them publicly with all partners.</li>
    <li>D. Check them into version control.</li>
  </ul>
  <details><summary>Show Answer</summary>
  <div class="answer">
    <strong>✅ Correct Answer:</strong> B — Rotate and store them in secure services such as Azure Key Vault.<br>
    <strong>Explanation:</strong> Protecting keys in managed secret stores reduces exposure and supports compliance requirements.
    Learn more in the <a href="https://learn.microsoft.com/azure/key-vault/general/basic-concepts">Microsoft documentation</a>.
  </div></details>
</div>

</body>
</html>
